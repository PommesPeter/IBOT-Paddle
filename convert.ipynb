{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:7)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import paddle\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import utils\n",
    "from models import vit_small, MultiCropWrapper, IBOTHead\n",
    "\n",
    "paddle.device.set_device(\"gpu:7\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddle.seed(10)\n",
    "\n",
    "student = vit_small(patch_size=16, drop_path_rate=0.1, return_all_tokens=True, masked_im_modeling=True)\n",
    "teacher = vit_small(patch_size=16, return_all_tokens=True)\n",
    "embed_dim = student.embed_dim\n",
    "student = MultiCropWrapper(student, IBOTHead(\n",
    "    embed_dim,\n",
    "    out_dim=8192,\n",
    "    patch_out_dim=8192,\n",
    "    norm=None,\n",
    "    act=\"gelu\",\n",
    "    norm_last_layer=False,\n",
    "    shared_head=True,\n",
    "))\n",
    "\n",
    "teacher = MultiCropWrapper(\n",
    "    teacher,\n",
    "    IBOTHead(\n",
    "        embed_dim,\n",
    "        out_dim=8192,\n",
    "        patch_out_dim=8192,\n",
    "        norm=None,\n",
    "        act=\"gelu\",\n",
    "        shared_head=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "paddle_weight = student.state_dict()\n",
    "student.eval()\n",
    "teacher.eval()\n",
    "\n",
    "torch_ckpt = torch.load(\"/home/xiejunlin/workspace/ibot/pretrained/checkpoint.pth\", map_location='cpu')\n",
    "torch_ckpt[\"student\"] = {k.replace(\"module.\", \"\"): v for k, v in torch_ckpt[\"student\"].items()}\n",
    "torch_ckpt[\"teacher\"] = {k.replace(\"module.\", \"\"): v for k, v in torch_ckpt[\"teacher\"].items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_weight = torch_ckpt[\"student\"]\n",
    "# student_weight.pop(\"head.last_layer.weight_g\")\n",
    "# student_weight.pop(\"head.last_layer2.weight_g\")\n",
    "# student_weight.pop('head.last_layer.weight_v')\n",
    "# student_weight.pop('head.last_layer2.weight_v')\n",
    "\n",
    "student_weight_dict = OrderedDict()\n",
    "for paddle_key in paddle_weight.keys():\n",
    "    # 首先要确保torch的权重里面有这个key，这样就可以避免DIY模型中一些小模块影响权重转换\n",
    "    if paddle_key in student_weight.keys():\n",
    "        # pytorch权重和paddle模型的权重为2维时需要转置，其余情况不需要\n",
    "        if len(student_weight[paddle_key].detach().numpy().shape) == 2 and \"masked_embed\" not in paddle_key:\n",
    "            # print(paddle_key)\n",
    "            student_weight_dict[paddle_key] = student_weight[paddle_key].detach().numpy().T\n",
    "        else:\n",
    "            student_weight_dict[paddle_key] = student_weight[paddle_key].detach().numpy()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "student_weight_dict[\"head.last_layer.weight_g\"] = student_weight[\"head.last_layer.weight_g\"].squeeze(-1).detach().cpu().numpy()\n",
    "student_weight_dict[\"head.last_layer2.weight_g\"] = student_weight[\"head.last_layer2.weight_g\"].squeeze(-1).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "teacher_weight = torch_ckpt[\"teacher\"]\n",
    "# teacher_weight.pop('head.last_layer.weight_g')\n",
    "# teacher_weight.pop('head.last_layer2.weight_g')\n",
    "# teacher_weight.pop('head.last_layer.weight_v')\n",
    "# teacher_weight.pop('head.last_layer2.weight_v')\n",
    "\n",
    "teacher_weight_dict = OrderedDict()\n",
    "for paddle_key in paddle_weight.keys():\n",
    "    # 首先要确保torch的权重里面有这个key，这样就可以避免DIY模型中一些小模块影响权重转换\n",
    "    if paddle_key in teacher_weight.keys():\n",
    "        # pytorch权重和paddle模型的权重为2维时需要转置，其余情况不需要\n",
    "        if len(teacher_weight[paddle_key].detach().numpy().shape) == 2 and \"masked_embed\" not in paddle_key:\n",
    "            # print(paddle_key)\n",
    "            teacher_weight_dict[paddle_key] = teacher_weight[paddle_key].detach().numpy().T\n",
    "        else:\n",
    "            teacher_weight_dict[paddle_key] = teacher_weight[paddle_key].detach().numpy()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "teacher_weight_dict[\"head.last_layer.weight_g\"] = student_weight[\"head.last_layer.weight_g\"].squeeze(-1).detach().cpu().numpy()\n",
    "teacher_weight_dict[\"head.last_layer2.weight_g\"] = student_weight[\"head.last_layer2.weight_g\"].squeeze(-1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch_ckpt['optimizer']\n",
    "optimizer.keys()\n",
    "\n",
    "optim_dict = {'state': {}, 'param_groups': []}\n",
    "\n",
    "for k, v in optimizer['state'].items():\n",
    "    optim_dict['state'][k] = {}\n",
    "\n",
    "for k, v in optimizer['state'].items():\n",
    "    for k2, v2 in v.items():\n",
    "        if isinstance(v2, torch.Tensor):\n",
    "            v2 = v2.detach().numpy()\n",
    "        optim_dict['state'][k][k2] = v2\n",
    "\n",
    "\n",
    "for param in optimizer['param_groups']:\n",
    "    optim_dict['param_groups'].append(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_groups = utils.get_params_groups(student)\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=0.0001, parameters=params_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ibot_loss = torch_ckpt['ibot_loss']\n",
    "ibot_loss = OrderedDict({\n",
    "    k: v.detach().numpy() for k, v in ibot_loss.items()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddle.save({\"student\": student_weight_dict, \"teacher\":teacher_weight_dict, \"epoch\": 100, \"ibot_loss\": ibot_loss, \"optimizer\": optim_dict}, \"check/ckpt/full_ckpt_v2.pdparams\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddle.save({\"student\": student_weight_dict, \"teacher\":teacher_weight_dict, \"epoch\": 100, \"ibot_loss\": ibot_loss}, \"check/ckpt/full_ckpt_weight_gv_key_v3.pdparams\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0796394 , 1.1381648 , 1.3803074 , ..., 1.3158706 , 0.95200276,\n",
       "       1.2740731 ], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "weight_g_pd = np.load(\"/home/xiejunlin/workspace/IBOT-Paddle/data/weight_norm_g_pd.npy\")\n",
    "weight_g_th = np.load(\"/home/xiejunlin/workspace/IBOT-Paddle/data/weight_norm_g_th.npy\")\n",
    "\n",
    "res = np.abs(weight_g_pd - weight_g_th).mean()\n",
    "weight_g_th\n",
    "weight_g_pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
